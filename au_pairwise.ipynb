{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About this code\n",
    "- This is a code for FG2020 ABAW AU detection Challenge.\n",
    "     - paper title: Action Unit Recognition by Pairwise Deep Architecture\n",
    "- Copyright 2020 FUJITSU LABORATORIES LTD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries to import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T13:50:10.148346Z",
     "start_time": "2020-07-30T13:50:09.089621Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torchvision import models, transforms, datasets\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import scipy\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters to need to set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T07:00:28.504009Z",
     "start_time": "2020-07-31T07:00:28.492265Z"
    }
   },
   "outputs": [],
   "source": [
    "au_name = \"AU01\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T07:00:28.504009Z",
     "start_time": "2020-07-31T07:00:28.492265Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus_top_dirname = \"../../corpus\"\n",
    "dataset_lst_dirname = \"./BinaryData\"\n",
    "\n",
    "phase_lst = [\"Train\",\"Valid\",\"Test\"]\n",
    "au_name_lst = [au_name]\n",
    "\n",
    "train_size_per_epoch = 2048*4\n",
    "epoch_num = 10\n",
    "dataset_num_lst = { \"Train\":train_size_per_epoch*epoch_num, \"Valid\":512, \"Test\":512 }\n",
    "intensity_conv_dataset_num_lst = { \"Train\":2048*10, \"Valid\":2048*2, \"Test\":2048*2, \"TestFull\":-1, \"TrainFull\":-1,\"ValidFull\":-1 }\n",
    "small_debug_mode = False\n",
    "intensity_tune_sampling_ratio = 1.0\n",
    "intensity_tune_sampling_min = 10000000\n",
    "trial_num = 5\n",
    "\n",
    "\n",
    "# train_size_per_epoch = 128\n",
    "# epoch_num = 2\n",
    "# dataset_num_lst = { \"Train\":train_size_per_epoch*epoch_num, \"Valid\":32, \"Test\":32 }\n",
    "# intensity_conv_dataset_num_lst = { \"Train\":128, \"Valid\":32, \"Test\":32, \"TestFull\":-1, \"TrainFull\":-1,\"ValidFull\":-1 }\n",
    "# small_debug_mode = True\n",
    "# intensity_tune_sampling_ratio = 0.0\n",
    "# intensity_tune_sampling_min = 10\n",
    "# trial_num = 2\n",
    "\n",
    "\n",
    "intensity_net_feature_lst = [ \"hist\", \"percentile\" ]\n",
    "\n",
    "map_location = 'cuda:0'\n",
    "\n",
    "batch_size_bin = 32\n",
    "batch_size_int = 32\n",
    "\n",
    "int_occ_mode = \"occ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if int_occ_mode == \"occ\":  \n",
    "\n",
    "    phase_full_lst = [\"ValidFull\", \"TestFull\"]\n",
    "    \n",
    "    pose_lst = [1]\n",
    "    \n",
    "    dataset_path_lst = {}\n",
    "    dataset_path_lst[\"Train\"] = {}\n",
    "    dataset_path_lst[\"Train\"][\"posesall\"] = {}\n",
    "    dataset_path_lst[\"Train\"][\"posesall\"][\"FERA\"] = [ (corpus_top_dirname + \"/FERA/tmp/frames_for_pytorch_procrustes_int_pose%d/Train\" % pose) for pose in [1,2,3,4,5,6,7,8,9] ]\n",
    "    dataset_path_lst[\"Train\"][\"posesall\"][\"FERA\"] += [ (corpus_top_dirname + \"/FERA_mirror/tmp/frames_for_pytorch_procrustes_int_pose%d/Train\" % pose) for pose in [1,2,3,4,5,6,7,8,9] ]\n",
    "    dataset_path_lst[\"Train\"][\"posesall\"][\"FERA\"] += [ (corpus_top_dirname + \"/FERA_pose/tmp/frames_for_pytorch_procrustes_int_pose%d/Train\" % pose) for pose in [1,2] ]\n",
    "    dataset_path_lst[\"Train\"][\"posesall\"][\"FERA\"] += [ (corpus_top_dirname + \"/FERA_pose_mirror/tmp/frames_for_pytorch_procrustes_int_pose%d/Train\" % pose) for pose in [1,2] ]\n",
    "   \n",
    "    \n",
    "    dataset_path_lst[\"Train\"][\"posesall\"][\"DISFA\"] = [ (corpus_top_dirname + \"/DISFA/tmp/frames_for_pytorch_procrustes_int_pose%d/Train\" % pose) for pose in [1,2] ]    \n",
    "    dataset_path_lst[\"Train\"][\"posesall\"][\"AffWild2\"] = [ (corpus_top_dirname + \"/AffWild2/tmp/frames_for_pytorch_procrustes_occ_pose%d/Train\" % pose) for pose in pose_lst ]\n",
    " \n",
    "    \n",
    "    dataset_path_lst[\"Valid\"] = { \"pose%d\" % pose : {\"AffWild2\":[(corpus_top_dirname + \"/AffWild2/tmp/frames_for_pytorch_procrustes_occ_pose%d/Valid\" % pose)]}  for pose in pose_lst }\n",
    "    dataset_path_lst[\"Test\"] = { \"pose%d\" % pose : {\"AffWild2\":[(corpus_top_dirname + \"/AffWild2/tmp/frames_for_pytorch_procrustes_occ_pose%d/Test\" % pose)]}  for pose in pose_lst }\n",
    "\n",
    "\n",
    "    intensity_tune_dataset_path_lst = {}\n",
    "\n",
    "    intensity_tune_dataset_path_lst[\"Train\"] = {}\n",
    "    intensity_tune_dataset_path_lst[\"Train\"][\"posesall\"] = {}\n",
    "    intensity_tune_dataset_path_lst[\"Train\"][\"posesall\"][\"AffWild2\"] = [ (corpus_top_dirname + \"/AffWild2/tmp/frames_for_pytorch_procrustes_occ_pose%d/Train\" % pose) for pose in pose_lst ]\n",
    "    \n",
    "\n",
    "    intensity_tune_dataset_path_lst[\"Valid\"] = { \"pose%d\" % pose : { \"AffWild2\": [(corpus_top_dirname + \"/AffWild2/tmp/frames_for_pytorch_procrustes_occ_pose%d/Valid\" % pose)]}  for pose in pose_lst }   \n",
    "    intensity_tune_dataset_path_lst[\"ValidFull\"] = { \"pose%d\" % pose : { \"AffWild2\": [(corpus_top_dirname + \"/AffWild2/tmp/frames_for_pytorch_procrustes_occ_pose%d/Valid\" % pose)]}  for pose in pose_lst }\n",
    "    \n",
    "    intensity_tune_dataset_path_lst[\"Test\"] = { \"pose%d\" % pose : { \"AffWild2\": [(corpus_top_dirname + \"/AffWild2/tmp/frames_for_pytorch_procrustes_occ_pose%d/Test\" % pose)]}  for pose in pose_lst }    \n",
    "    intensity_tune_dataset_path_lst[\"TestFull\"] = { \"pose%d\" % pose : { \"AffWild2\": [(corpus_top_dirname + \"/AffWild2/tmp/frames_for_pytorch_procrustes_occ_pose%d/Test\" % pose)]}  for pose in pose_lst }\n",
    "    \n",
    "   \n",
    "    enable_pair_dataset_equal_test = True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T13:50:10.183445Z",
     "start_time": "2020-07-30T13:50:10.172151Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(map_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition of functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T13:50:10.202172Z",
     "start_time": "2020-07-30T13:50:10.184895Z"
    }
   },
   "outputs": [],
   "source": [
    "def init_dir(dn):\n",
    "    if os.path.exists(dn):\n",
    "        shutil.rmtree(dn)\n",
    "    os.makedirs(dn)\n",
    "\n",
    "\n",
    "def measure_map( label_lst,pred_lst ):\n",
    "    \n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    TN = 0\n",
    "    \n",
    "    for i in range(len(label_lst)):\n",
    "\n",
    "        if label_lst[i] == 1:     \n",
    "            if pred_lst[i] == 1:\n",
    "                TP += 1\n",
    "            elif pred_lst[i] == 0:\n",
    "                FN += 1\n",
    "            else:\n",
    "                raise\n",
    "        elif label_lst[i] == 0:         \n",
    "            if pred_lst[i] == 1:\n",
    "                FP += 1\n",
    "            elif pred_lst[i] == 0:\n",
    "                TN += 1\n",
    "            else:\n",
    "                raise\n",
    "        else:\n",
    "            raise                \n",
    "                \n",
    "                \n",
    "    return TP,FP,FN,TN\n",
    "\n",
    "\n",
    "def PRECISION( TP,FP,FN,TN ):    \n",
    "    \n",
    "    if TP + FP == 0:\n",
    "        return 0.0\n",
    "        # return np.nan\n",
    "    \n",
    "\n",
    "    return TP / ( TP + FP )\n",
    "\n",
    "\n",
    "def RECALL( TP,FP,FN,TN ):\n",
    "\n",
    "    if TP + FN == 0:\n",
    "        return 0.0\n",
    "        # return np.nan    \n",
    "    \n",
    "    return TP / ( TP + FN )\n",
    "\n",
    "def ACCURACY( TP,FP,FN,TN ):\n",
    "\n",
    "    if TP + FP + TN + FN == 0:\n",
    "        return 0.0\n",
    "        # return np.nan    \n",
    "    \n",
    "    return (TP+TN) / ( TP + FP + TN + FN )\n",
    "\n",
    "def F1( TP,FP,FN,TN ):\n",
    "    \n",
    "    recall = RECALL( TP,FP,FN,TN )\n",
    "    precision = PRECISION( TP,FP,FN,TN )\n",
    "    \n",
    "    if recall + precision == 0:\n",
    "        return 0.0\n",
    "        # return np.nan\n",
    "\n",
    "    return 2 * recall * precision / ( recall + precision )\n",
    "\n",
    "if int_occ_mode == \"occ\":\n",
    "    \n",
    "    intensity_level = 2\n",
    "\n",
    "    def get_label_name( intensity ):\n",
    "        if intensity == 0:\n",
    "            return \"NOOCC\"\n",
    "        elif intensity == 1:\n",
    "            return \"OCC\"\n",
    "        else:\n",
    "            raise\n",
    "            \n",
    "    measurement_name = \"F1\"\n",
    "    \n",
    "    def get_measurement_score( label_lst, pred_lst ):\n",
    "        TP,FP,FN,TN = measure_map( label_lst,pred_lst )\n",
    "        \n",
    "        return F1( TP,FP,FN,TN )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T13:50:10.202172Z",
     "start_time": "2020-07-30T13:50:10.184895Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_img( img_name_0 ):\n",
    "    with open(img_name_0, 'rb') as f:\n",
    "        img_0 = Image.open(f)\n",
    "        img_0 = img_0.convert('RGB')\n",
    "\n",
    "    img_0 = transforms.ToTensor()(img_0)\n",
    "    img_0 = transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])(img_0)\n",
    "    \n",
    "    return img_0\n",
    "\n",
    "def get_best_epoch( filename, category, direct, dataset_path_lst ):\n",
    "\n",
    "    fp = open(filename,\"r\")\n",
    "    epoch_score_lst = json.load( fp )\n",
    "    fp.close()\n",
    "\n",
    "    pose_lst = list(dataset_path_lst[\"Valid\"].keys())\n",
    "    best_epoch = 0\n",
    "    for epoch in range(len(epoch_score_lst[\"Valid\"][pose_lst[0]][category])):\n",
    "\n",
    "        score_cur_best = np.average([epoch_score_lst[\"Valid\"][pose][category][best_epoch] for pose in pose_lst])\n",
    "        score = np.average([epoch_score_lst[\"Valid\"][pose][category][epoch] for pose in pose_lst])\n",
    "        \n",
    "        if direct == \"UPPER\":\n",
    "            if score_cur_best < score:\n",
    "                best_epoch = epoch\n",
    "        elif direct == \"LOWER\":\n",
    "            if score_cur_best > score:\n",
    "                best_epoch = epoch\n",
    "                \n",
    "                \n",
    "                \n",
    "    print(\"best_epoch\",best_epoch)\n",
    "      \n",
    "    print(\"---\")\n",
    "    \n",
    "    for phase in epoch_score_lst.keys():\n",
    "        pose_lst = list(dataset_path_lst[phase].keys())\n",
    "        for c in epoch_score_lst[phase][pose_lst[0]].keys():\n",
    "            \n",
    "            score_lst = [epoch_score_lst[phase][pose][c][best_epoch] for pose in pose_lst]\n",
    "            print(phase,c,\"AVERAGE:\",np.average(score_lst),\"STD:\",np.std(score_lst))\n",
    "            \n",
    "    return best_epoch\n",
    "\n",
    "def show_score_lst( filename, dataset_path_lst ):\n",
    "\n",
    "    fp = open(filename,\"r\")\n",
    "    epoch_score_lst = json.load( fp )\n",
    "    fp.close()\n",
    "    \n",
    "    pose_lst = list(dataset_path_lst[\"Valid\"].keys())\n",
    "    for c in epoch_score_lst[\"Valid\"][pose_lst[0]].keys():\n",
    "        for phase in epoch_score_lst.keys():\n",
    "            pose_lst = list(dataset_path_lst[phase].keys())\n",
    "            en = len(epoch_score_lst[phase][pose_lst[0]][c])\n",
    "            score_lst = [ np.average([ epoch_score_lst[phase][pose][c][epoch] for pose in pose_lst ]) for epoch in range(en) ]\n",
    "            \n",
    "            plt.plot(list(range(len(score_lst))),score_lst,label=\"%s %s\" % (phase,c))\n",
    "\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0)\n",
    "        plt.title(c)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T00:28:59.048851Z",
     "start_time": "2020-05-07T00:28:59.046144Z"
    }
   },
   "source": [
    "# Initialize directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T13:50:10.227173Z",
     "start_time": "2020-07-30T13:50:10.204632Z"
    }
   },
   "outputs": [],
   "source": [
    "init_dir(dataset_lst_dirname)\n",
    "init_dir(\"./pred_result\")\n",
    "init_dir(\"./log\")\n",
    "init_dir(\"./model\")\n",
    "init_dir(\"./tmp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T13:50:10.250281Z",
     "start_time": "2020-07-30T13:50:10.236130Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_filename_lst_cache(dataset_path_lst,au_name):\n",
    "    \n",
    "    filename_lst_cache_all = []\n",
    "    for dataset_name,dataset_path_ll in dataset_path_lst.items():\n",
    "        filename_lst_cache = {}\n",
    "        occ_int_flag = \"int\"\n",
    "        pose_mode = \"single_pose\"\n",
    "        pose_prev = None\n",
    "        \n",
    "        for intensity in range(6):\n",
    "            filename_lst_cache[intensity] = {}\n",
    "            \n",
    "            for dataset_path in dataset_path_ll:    \n",
    "\n",
    "\n",
    "                if dataset_path.find( \"frames_for_pytorch_procrustes_occ_pose\" ) >= 0:\n",
    "\n",
    "                    occ_int_flag = \"occ\"\n",
    "\n",
    "                    if intensity > 1:\n",
    "                        continue\n",
    "\n",
    "                    dirname = dataset_path + \"/\" + au_name + \"/\" + ( \"NOOCC\" if intensity == 0 else \"OCC\" )\n",
    "\n",
    "                elif  dataset_path.find( \"frames_for_pytorch_procrustes_int_pose\" ) >= 0:\n",
    "\n",
    "                    dirname = dataset_path + \"/\" + au_name + \"/\" + (\"INT%d\" % intensity)\n",
    "                else:\n",
    "                    raise\n",
    "\n",
    "                for filename in sorted(glob.glob(dirname + \"/*.jpg\")):\n",
    "                    subject = os.path.basename(filename).split(\"_\")[2] + \"_\" + os.path.basename(filename).split(\"_\")[3]\n",
    "                    pose = int(os.path.basename(filename).split(\"_\")[4])\n",
    "                    frame_id = int(os.path.splitext(os.path.basename(filename))[0].split(\"_\")[5][len(\"frame\"):])\n",
    "\n",
    "                    if (not pose_prev is None) and (pose != pose_prev):\n",
    "                        pose_mode = \"multi_pose\"\n",
    "                    pose_prev = pose\n",
    "\n",
    "                    if not subject in filename_lst_cache[intensity]:\n",
    "                        filename_lst_cache[intensity][subject] = [[],[],[]]\n",
    "\n",
    "                    filename_lst_cache[intensity][subject][0].append(filename)\n",
    "                    filename_lst_cache[intensity][subject][1].append(pose)\n",
    "                    filename_lst_cache[intensity][subject][2].append(frame_id)\n",
    "                \n",
    "        filename_lst_cache_all.append((filename_lst_cache,occ_int_flag,pose_mode,dataset_name))\n",
    "        \n",
    "    return filename_lst_cache_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make csv file of dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T00:28:14.108221Z",
     "start_time": "2020-05-07T00:28:14.105715Z"
    }
   },
   "source": [
    "### for training pseudo-intensity model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T13:51:15.303892Z",
     "start_time": "2020-07-30T13:50:19.330601Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for au_name in au_name_lst:    \n",
    "    for phase in phase_lst:\n",
    "        for pose in dataset_path_lst[phase].keys():\n",
    "        \n",
    "            csv_filename = dataset_lst_dirname + \"/AU_INT_PAIR_%s_%s_%s.csv\" % (au_name,phase,pose)\n",
    "\n",
    "            print(\"---------------------\")\n",
    "            \n",
    "            print(csv_filename)\n",
    "\n",
    "            filename_lst_cache_all = get_filename_lst_cache(dataset_path_lst[phase][pose],au_name)\n",
    "\n",
    "            \n",
    "            for i in range(len(filename_lst_cache_all)):\n",
    "                print(filename_lst_cache_all[i][1],filename_lst_cache_all[i][2],filename_lst_cache_all[i][3])\n",
    "\n",
    "            fp = open(csv_filename,\"w\")\n",
    "\n",
    "            counter = 0\n",
    "            while counter < dataset_num_lst[phase]:\n",
    "\n",
    "                dataset_id = random.sample(list(range(len(filename_lst_cache_all))),1)[0]\n",
    "                filename_lst_cache,occ_int_flag,pose_mode,_ = filename_lst_cache_all[dataset_id]\n",
    "\n",
    "                intensity_pair_lst = []\n",
    "                for intensity_0 in range(2 if occ_int_flag == \"occ\" else 6):\n",
    "                    for intensity_1 in range(2 if occ_int_flag == \"occ\" else 6):\n",
    "\n",
    "                        if enable_pair_dataset_equal_test and phase == \"Test\":\n",
    "                            pass\n",
    "                        \n",
    "                        else:                        \n",
    "                            if abs(intensity_0 - intensity_1) == 0:\n",
    "                                continue\n",
    "                        \n",
    "                        intensity_pair_lst.append((intensity_0,intensity_1))\n",
    "                        \n",
    "                target_intensity_pair = random.sample(intensity_pair_lst,1)[0]               \n",
    "                \n",
    "                \n",
    "                intensity_0 = target_intensity_pair[0]\n",
    "                intensity_1 = target_intensity_pair[1]\n",
    "\n",
    "\n",
    "                if intensity_0 < intensity_1:\n",
    "                    label = 1\n",
    "                elif intensity_0 > intensity_1:\n",
    "                    label = 0\n",
    "                else:\n",
    "                    if enable_pair_dataset_equal_test and phase == \"Test\":\n",
    "                        label = 0\n",
    "                    else:\n",
    "                        raise\n",
    "\n",
    "\n",
    "                subject_lst_0 = sorted(list(filename_lst_cache[intensity_0].keys()))\n",
    "                subject_lst_1 = sorted(list(filename_lst_cache[intensity_1].keys()))\n",
    "                                \n",
    "\n",
    "                if len(subject_lst_0) == 0:\n",
    "                    continue\n",
    "                if len(subject_lst_1) == 0:\n",
    "                    continue\n",
    "\n",
    "                pair_lst = []\n",
    "                for s0 in subject_lst_0:\n",
    "                    for s1 in subject_lst_1:\n",
    "                        if s0 == s1:\n",
    "                            pair_lst.append((s0,s1))                          \n",
    "\n",
    "\n",
    "                if len(pair_lst) == 0:\n",
    "                    continue\n",
    "\n",
    "\n",
    "                subject_0,subject_1 = random.sample(pair_lst,1)[0]\n",
    "\n",
    "\n",
    "                filename_lst_0,pose_lst_0,frame_id_lst_0 = filename_lst_cache[intensity_0][subject_0]\n",
    "                filename_lst_1,pose_lst_1,frame_id_lst_1 = filename_lst_cache[intensity_1][subject_1]\n",
    "\n",
    "                if len(filename_lst_0) == 0:\n",
    "                    continue\n",
    "                if len(filename_lst_1) == 0:\n",
    "                    continue\n",
    "\n",
    "                filename_0 = random.sample(filename_lst_0,1)[0]\n",
    "                filename_1 = random.sample(filename_lst_1,1)[0]\n",
    "\n",
    "                fp.write(\"%s,%s,%d\\n\" % (os.path.abspath(filename_0),os.path.abspath(filename_1),label))\n",
    "                counter += 1\n",
    "\n",
    "\n",
    "            fp.close()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T00:27:25.944876Z",
     "start_time": "2020-05-07T00:27:25.942433Z"
    }
   },
   "source": [
    "### for training mapping model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T01:17:13.558831Z",
     "start_time": "2020-07-31T01:16:53.350567Z"
    }
   },
   "outputs": [],
   "source": [
    "for au_name in au_name_lst:    \n",
    "    for phase in phase_lst:\n",
    "        for pose in intensity_tune_dataset_path_lst[phase].keys():\n",
    "            csv_filename = dataset_lst_dirname + \"/AU_INT_TUNE_%s_%s_%s.csv\" % (au_name,phase,pose)\n",
    "\n",
    "            print(csv_filename)\n",
    "\n",
    "            fp = open(csv_filename,\"w\")\n",
    "            \n",
    "            for dataset_name in intensity_tune_dataset_path_lst[phase][pose].keys():\n",
    "\n",
    "                filename_lst = []\n",
    "                for cdirname in intensity_tune_dataset_path_lst[phase][pose][dataset_name]:\n",
    "                    for intensity in range(intensity_level):\n",
    "                        for filename in sorted(glob.glob(cdirname + \"/\" + au_name + \"/\" + get_label_name(intensity) + \"/*.jpg\")):\n",
    "\n",
    "                            if small_debug_mode:\n",
    "                                if random.random() > 0.01:\n",
    "                                    continue\n",
    "\n",
    "                            fp.write(\"%s,%d\\n\" % (os.path.abspath(filename),intensity))\n",
    "           \n",
    "            fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T01:17:13.558831Z",
     "start_time": "2020-07-31T01:16:53.350567Z"
    }
   },
   "outputs": [],
   "source": [
    "for au_name in au_name_lst:    \n",
    "    for phase in phase_lst:\n",
    "        for pose in intensity_tune_dataset_path_lst[phase].keys():\n",
    "            csv_filename = dataset_lst_dirname + \"/AU_INT_TUNE_MINI_%s_%s_%s.csv\" % (au_name,phase,pose)\n",
    "\n",
    "            print(csv_filename)\n",
    "\n",
    "            fp = open(csv_filename,\"w\")\n",
    "            \n",
    "            for dataset_name in intensity_tune_dataset_path_lst[phase][pose].keys():\n",
    "\n",
    "                for cdirname in intensity_tune_dataset_path_lst[phase][pose][dataset_name]:\n",
    "                    filename_lst = {}\n",
    "                    \n",
    "                    for intensity in range(intensity_level):\n",
    "                        for filename in sorted(glob.glob(cdirname + \"/\" + au_name + \"/\" + get_label_name(intensity) + \"/*.jpg\")):\n",
    "                            \n",
    "                            subject_task_pose = os.path.basename(filename).split(\"_\")[2] + \"_\" + os.path.basename(filename).split(\"_\")[3] + \"_\" + os.path.basename(filename).split(\"_\")[4]\n",
    "                            if not subject_task_pose in filename_lst:\n",
    "                                filename_lst[subject_task_pose] = []\n",
    "                            filename_lst[subject_task_pose].append((filename,intensity))\n",
    "                    \n",
    "\n",
    "                    for subject_task_pose, filename_intensity_lst in filename_lst.items():\n",
    "\n",
    "                        sampling_num = min(max(int(len(filename_intensity_lst) * intensity_tune_sampling_ratio),intensity_tune_sampling_min),len(filename_intensity_lst))\n",
    "                        \n",
    "                        for filename,intensity in random.sample(filename_intensity_lst,sampling_num):\n",
    "                            fp.write(\"%s,%d\\n\" % (os.path.abspath(filename),intensity))\n",
    "            fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train pseudo-intensity model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T00:26:18.747187Z",
     "start_time": "2020-05-07T00:26:18.744640Z"
    }
   },
   "source": [
    "### Define dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T13:51:26.502319Z",
     "start_time": "2020-07-30T13:51:26.494693Z"
    }
   },
   "outputs": [],
   "source": [
    "class PairDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, csv_file):\n",
    "        self.df = pd.read_csv(csv_file,header=None)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name_0 = self.df.iloc[idx,0]\n",
    "        img_name_1 = self.df.iloc[idx,1]\n",
    "        label = self.df.iloc[idx,2]\n",
    "       \n",
    "        img_0 = load_img( img_name_0 )\n",
    "        img_1 = load_img( img_name_1 )\n",
    "\n",
    "        return img_0,img_1,label,img_name_0,img_name_1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T13:51:26.697010Z",
     "start_time": "2020-07-30T13:51:26.504145Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_dataloaders_for_pairwise_net( dataset_path_lst, dataset_lst_dirname, au_name_lst, phase_lst, batch_size_bin ):\n",
    "\n",
    "    dataloaders = {}\n",
    "\n",
    "    for au_name in au_name_lst:\n",
    "        dataloaders[au_name] = {}\n",
    "        for phase in phase_lst:\n",
    "            dataloaders[au_name][phase] = {}\n",
    "            for pose in dataset_path_lst[phase].keys():\n",
    "\n",
    "                csv_filename = dataset_lst_dirname + \"/AU_INT_PAIR_%s_%s_%s.csv\" % (au_name,phase,pose)\n",
    "\n",
    "                pair_dataset = PairDataset(csv_filename)\n",
    "                dataloader = torch.utils.data.DataLoader(pair_dataset, batch_size=batch_size_bin, shuffle=True, num_workers=1)\n",
    "\n",
    "                dataloaders[au_name][phase][pose] = dataloader\n",
    "                \n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T13:51:26.709091Z",
     "start_time": "2020-07-30T13:51:26.698547Z"
    }
   },
   "outputs": [],
   "source": [
    "class PairNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PairNet, self).__init__()\n",
    "        \n",
    "        dropout = 0.5\n",
    "\n",
    "        self.model = models.vgg16(pretrained=True)\n",
    "        \n",
    "\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if ('features.0.' in name or 'features.2.' in name):\n",
    "                param.requires_grad = False                    \n",
    "\n",
    "\n",
    "        self.model.classifier[2].p = dropout\n",
    "        self.model.classifier[5].p = dropout\n",
    "\n",
    "        num_ftrs = self.model.classifier[6].in_features\n",
    "        self.model.classifier[6] = nn.Linear(num_ftrs, 1)\n",
    "\n",
    "    def forward(self, img_0, img_1):\n",
    "        \n",
    "        feature_0 = self.model.forward(img_0)\n",
    "        feature_1 = self.model.forward(img_1)\n",
    "        \n",
    "        x_0 = torch.add(torch.mul(feature_1,-1),feature_0)\n",
    "        x_1 = torch.add(torch.mul(feature_0,-1),feature_1)        \n",
    "        \n",
    "        x = torch.cat([x_0,x_1],dim=1)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "\n",
    "    def forward_single(self, img_0):\n",
    "        \n",
    "        feature_0 = self.model.forward(img_0)\n",
    "\n",
    "        return feature_0\n",
    "    \n",
    "def my_loss_func(output,label):\n",
    "\n",
    "    output = output[:,0]\n",
    "    label = torch.add(torch.mul(label,2),-1)\n",
    "    m = 1    \n",
    "    loss_element = torch.clamp(torch.add(torch.mul( output, label ),m), min=0)\n",
    "    loss = torch.mean(loss_element)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T14:58:48.917748Z",
     "start_time": "2020-07-30T13:51:26.711155Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_pairwise_net( dataloaders, dataset_path_lst, au_name_lst, phase_lst, device, train_size_per_epoch ):\n",
    "\n",
    "    for au_name in au_name_lst:\n",
    "\n",
    "        print(\"=====================\")\n",
    "        print(\"au_name\",au_name)\n",
    "\n",
    "        pair_net = PairNet()\n",
    "        pair_net = pair_net.to(device)\n",
    "        optimizer = optim.Adam(pair_net.parameters(), lr=0.00005)\n",
    "\n",
    "        epoch_score_lst = { phase: { pose: { category: [] for category in [\"ACC\",\"LOSS\"] } for pose in dataset_path_lst[phase].keys() } for phase in phase_lst }\n",
    "\n",
    "\n",
    "\n",
    "        total_train_size = 0\n",
    "        epoch = 0    \n",
    "\n",
    "        # score\n",
    "        running_corrects = 0\n",
    "        total_input_size = 0\n",
    "        running_loss = 0\n",
    "\n",
    "        # Train flag\n",
    "        pair_net.train()\n",
    "        pair_net.model.train()\n",
    "\n",
    "\n",
    "        for img_0_batch, img_1_batch, label_batch, path_0_batch, path_1_batch in dataloaders[au_name][\"Train\"][\"posesall\"]:\n",
    "\n",
    "            # Train\n",
    "            img_0_batch = img_0_batch.to(device)\n",
    "            img_1_batch = img_1_batch.to(device)\n",
    "            label_batch = label_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(True):\n",
    "\n",
    "                output_batch = pair_net.forward(img_0_batch,img_1_batch)\n",
    "                _, pred_batch = torch.max(output_batch, 1)\n",
    "\n",
    "                loss = my_loss_func(output_batch, label_batch) # criterion(output_batch, label_batch)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * img_0_batch.size(0)\n",
    "                running_corrects += torch.sum(pred_batch == label_batch.data)\n",
    "                total_input_size += img_0_batch.size(0)\n",
    "\n",
    "\n",
    "            total_train_size += img_0_batch.size(0)\n",
    "\n",
    "            # Valid, Test\n",
    "            if total_train_size >= (epoch + 1) * train_size_per_epoch:\n",
    "\n",
    "                print(\"---------------------\")\n",
    "\n",
    "                print(\"phase\",\"Train\")\n",
    "                print(\"epoch\",epoch)\n",
    "                print(\"train_size\",total_train_size)\n",
    "\n",
    "                epoch_acc = float((running_corrects.double()  / total_input_size).to('cpu').detach().numpy().copy())\n",
    "                epoch_loss = running_loss / total_input_size\n",
    "\n",
    "                print(\"epoch_acc\",epoch_acc)\n",
    "                print(\"epoch_loss\",epoch_loss)\n",
    "\n",
    "                epoch_score_lst[\"Train\"][\"posesall\"][\"ACC\"].append(epoch_acc)\n",
    "                epoch_score_lst[\"Train\"][\"posesall\"][\"LOSS\"].append(epoch_loss)\n",
    "\n",
    "\n",
    "\n",
    "                # Eval flag\n",
    "                pair_net.eval()\n",
    "                pair_net.model.eval()\n",
    "\n",
    "                for phase in [\"Valid\",\"Test\"]:\n",
    "                    for pose in dataset_path_lst[phase].keys():\n",
    "\n",
    "                        print(\"---\")\n",
    "\n",
    "                        print(\"phase\",phase)\n",
    "                        print(\"epoch\",epoch)\n",
    "                        print(\"pose\",pose)\n",
    "                        print(\"train_size\",total_train_size)\n",
    "\n",
    "                        # save\n",
    "                        fp_pred_result = open(\"./pred_result/pair_net_%s_epoch_%d_phase_%s_%s.csv\" % (au_name,epoch,phase,pose),\"w\")\n",
    "                        fp_pred_result.write(\"path_0,path_1,label,pred\\n\")\n",
    "\n",
    "\n",
    "\n",
    "                        running_corrects = 0\n",
    "                        total_input_size = 0\n",
    "                        running_loss = 0\n",
    "\n",
    "                        pred_lst = []\n",
    "                        label_lst = []\n",
    "                        path_0_lst = []\n",
    "                        path_1_lst = []\n",
    "                        prev_size = 0\n",
    "\n",
    "                        for img_0_batch, img_1_batch, label_batch, path_0_batch, path_1_batch in dataloaders[au_name][phase][pose]:\n",
    "\n",
    "                            img_0_batch = img_0_batch.to(device)\n",
    "                            img_1_batch = img_1_batch.to(device)\n",
    "                            label_batch = label_batch.to(device)\n",
    "\n",
    "                            optimizer.zero_grad()\n",
    "\n",
    "                            with torch.set_grad_enabled(False):\n",
    "\n",
    "                                output_batch = pair_net.forward(img_0_batch,img_1_batch)\n",
    "                                _, pred_batch = torch.max(output_batch, 1)\n",
    "\n",
    "                                loss = my_loss_func(output_batch, label_batch) # criterion(output_batch, label_batch)\n",
    "\n",
    "                            running_loss += loss.item() * img_0_batch.size(0)\n",
    "                            running_corrects += torch.sum(pred_batch == label_batch.data)\n",
    "                            total_input_size += img_0_batch.size(0)\n",
    "\n",
    "                            pred_lst += list(pred_batch.to('cpu').detach().numpy().copy())\n",
    "                            label_lst += list(label_batch.to('cpu').detach().numpy().copy())\n",
    "                            path_0_lst += list(path_0_batch)                       \n",
    "                            path_1_lst += list(path_1_batch)  \n",
    "\n",
    "\n",
    "                            # save\n",
    "                            for i in range(prev_size,len(path_0_lst)):\n",
    "                                fp_pred_result.write(\"%s,%s,%d,%d\\n\" % (path_0_lst[i],path_1_lst[i],int(label_lst[i]),int(pred_lst[i])) )                \n",
    "                            prev_size = len(path_0_lst)\n",
    "\n",
    "\n",
    "                        epoch_acc = float((running_corrects.double()  / total_input_size).to('cpu').detach().numpy().copy())\n",
    "                        epoch_loss = running_loss / total_input_size\n",
    "\n",
    "                        print(\"epoch_acc\",epoch_acc)\n",
    "                        print(\"epoch_loss\",epoch_loss)\n",
    "\n",
    "                        fp_pred_result.close()\n",
    "\n",
    "                        epoch_score_lst[phase][pose][\"ACC\"].append(epoch_acc)\n",
    "                        epoch_score_lst[phase][pose][\"LOSS\"].append(epoch_loss)     \n",
    "\n",
    "\n",
    "                # save\n",
    "                fp = open(\"./log/pair_net_epoch_score_lst_%s_%s.json\" % (au_name, \"posesall\") ,\"w\")\n",
    "                json.dump( epoch_score_lst, fp )\n",
    "                fp.close()\n",
    "\n",
    "\n",
    "\n",
    "                # score\n",
    "                running_corrects = 0\n",
    "                total_input_size = 0\n",
    "                running_loss = 0\n",
    "\n",
    "\n",
    "                # Train flag\n",
    "                pair_net.train()\n",
    "                pair_net.model.train()\n",
    "\n",
    "                # save\n",
    "                torch.save(pair_net.state_dict(), \"./model/pair_net_%s_epoch_%d_%s.pt\" % (au_name, epoch, \"posesall\" ) )\n",
    "\n",
    "                epoch += 1\n",
    "                \n",
    "        del pair_net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T14:58:49.376372Z",
     "start_time": "2020-07-30T14:58:48.937066Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_pairwise_net_best_epoch_lst( dataset_path_lst, au_name_lst ):\n",
    "\n",
    "    pair_net_best_epoch_lst = {}\n",
    "\n",
    "    for au_name in au_name_lst:\n",
    "        print(\"=============\")\n",
    "        print(au_name)\n",
    "\n",
    "        filename = \"./log/pair_net_epoch_score_lst_%s_%s.json\" % (au_name,\"posesall\")\n",
    "\n",
    "        pair_net_best_epoch_lst[au_name] = get_best_epoch( filename, \"LOSS\", \"LOWER\", dataset_path_lst )\n",
    "        show_score_lst( filename, dataset_path_lst )\n",
    "\n",
    "    return pair_net_best_epoch_lst\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Free memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T14:58:49.388445Z",
     "start_time": "2020-07-30T14:58:49.378410Z"
    }
   },
   "outputs": [],
   "source": [
    "def memory_free_pairwise_net_dataloaders( dataloaders ):\n",
    "    for au_name in dataloaders.keys():\n",
    "        for phase in dataloaders[au_name].keys():\n",
    "            for pose in list(dataloaders[au_name][phase].keys()):\n",
    "                del dataloaders[au_name][phase][pose]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T11:15:20.295093Z",
     "start_time": "2020-07-30T11:15:20.292396Z"
    }
   },
   "source": [
    "## Predict pseudo-intensity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T01:17:16.199421Z",
     "start_time": "2020-07-31T01:17:13.560720Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dataset\n",
    "class IntensityTuneDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, csv_file):\n",
    "        self.df = pd.read_csv(csv_file,header=None)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name_0 = self.df.iloc[idx,0]\n",
    "        label = self.df.iloc[idx,1]\n",
    "       \n",
    "        img_0 = load_img( img_name_0 )\n",
    "\n",
    "        return img_0,label,img_name_0\n",
    "\n",
    "\n",
    "def get_dataloaders_for_intensity_tune( intensity_tune_dataset_path_lst, au_name_lst, phase_lst, batch_size_int, is_mini ):\n",
    "\n",
    "    dataloaders_intensity_tune = {}\n",
    "\n",
    "    for au_name in au_name_lst:\n",
    "        dataloaders_intensity_tune[au_name] = {}\n",
    "        for phase in phase_lst:\n",
    "            dataloaders_intensity_tune[au_name][phase] = {}\n",
    "            for pose in intensity_tune_dataset_path_lst[phase].keys():\n",
    "\n",
    "                csv_filename = dataset_lst_dirname + \"/AU_INT_TUNE%s_%s_%s_%s.csv\" % ( (\"_MINI\" if is_mini else \"\"), au_name,phase,pose)\n",
    "\n",
    "                intensity_tune_dataset = IntensityTuneDataset(csv_filename)\n",
    "                dataloader = torch.utils.data.DataLoader(intensity_tune_dataset, batch_size=batch_size_int, shuffle=True, num_workers=1)\n",
    "\n",
    "                dataloaders_intensity_tune[au_name][phase][pose] = dataloader\n",
    "    return dataloaders_intensity_tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run predicting process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T04:32:53.108954Z",
     "start_time": "2020-07-31T01:17:16.201748Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_pairwise_net( pair_net_best_filename_lst, intensity_tune_dataset_path_lst, dataloaders_intensity_tune, au_name_lst, phase_lst, device, is_mini ):\n",
    "\n",
    "    for au_name in au_name_lst:\n",
    "\n",
    "        pair_net = PairNet()\n",
    "        pair_net.load_state_dict(torch.load(pair_net_best_filename_lst[au_name]))\n",
    "        pair_net.eval()\n",
    "        pair_net.to(device)\n",
    "\n",
    "        for phase in phase_lst:\n",
    "\n",
    "            print(\"------------\")\n",
    "            print(au_name,phase)\n",
    "\n",
    "            for pose in intensity_tune_dataset_path_lst[phase].keys():\n",
    "                print(\"phase\",phase,\"pose\",pose)\n",
    "\n",
    "                pred_lst_pose = []\n",
    "                label_lst_pose = []\n",
    "                path_lst_pose = []\n",
    "\n",
    "                for img_0_batch, label_batch, path_batch in dataloaders_intensity_tune[au_name][phase][pose]:\n",
    "\n",
    "                    img_0_batch = img_0_batch.to(device)\n",
    "                    with torch.set_grad_enabled(False):\n",
    "\n",
    "                        output_batch = pair_net.forward_single(img_0_batch)\n",
    "\n",
    "                    pred_batch = output_batch.to('cpu').detach().numpy().copy()\n",
    "\n",
    "                    pred_lst_pose += list(pred_batch)\n",
    "                    label_lst_pose += list(label_batch)\n",
    "                    path_lst_pose += list(path_batch)\n",
    "\n",
    "\n",
    "                fp_pred_result_raw = open(\"./pred_result/pred_result__%s_procrustes-intensity-%s_%s_psedo%s.csv\" % (au_name,pose,phase, \"_mini\" if is_mini else \"\" ),\"w\")\n",
    "                fp_pred_result_raw.write(\"path,label,pred\\n\")\n",
    "\n",
    "                for i in range(len(path_lst_pose)):\n",
    "                    fp_pred_result_raw.write(\"%s,%d,%f\\n\"%(path_lst_pose[i],label_lst_pose[i],pred_lst_pose[i]))\n",
    "\n",
    "                fp_pred_result_raw.close()            \n",
    "\n",
    "\n",
    "        # free menory\n",
    "        del pair_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Free memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T04:32:53.267341Z",
     "start_time": "2020-07-31T04:32:53.112132Z"
    }
   },
   "outputs": [],
   "source": [
    "def memeory_free_dataloaders_intensity_tune( dataloaders_intensity_tune ):\n",
    "\n",
    "    for au_name in dataloaders_intensity_tune.keys():\n",
    "        for phase in dataloaders_intensity_tune[au_name].keys():\n",
    "            for pose in list(dataloaders_intensity_tune[au_name][phase].keys()):\n",
    "                del dataloaders_intensity_tune[au_name][phase][pose]\n",
    "\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train mapping model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make csv file of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T07:00:39.171337Z",
     "start_time": "2020-07-31T07:00:32.908141Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_dataset_for_intensity_net( intensity_tune_dataset_path_lst, intensity_conv_dataset_num_lst, dataset_lst_dirname, au_name_lst, phase_lst, phase_full_lst, is_mini ):\n",
    "\n",
    "    for au_name in au_name_lst:    \n",
    "        for phase in phase_lst + phase_full_lst:\n",
    "            for pose in intensity_tune_dataset_path_lst[phase].keys():\n",
    "\n",
    "                phase_tmp = phase\n",
    "                if phase_tmp == \"TrainFull\":\n",
    "                    phase_tmp = \"Train\"\n",
    "                if phase_tmp == \"ValidFull\":\n",
    "                    phase_tmp = \"Valid\"\n",
    "                if phase_tmp == \"TestFull\":\n",
    "                    phase_tmp = \"Test\"\n",
    "\n",
    "                csv_filename = \"./pred_result/pred_result__%s_procrustes-intensity-%s_%s_psedo%s.csv\" % (au_name,pose,phase_tmp,\"_mini\" if is_mini else \"\")\n",
    "\n",
    "                dataset_lst = { dataset_name:[os.path.abspath(dataset_path) for dataset_path in _dataset_path_lst] for dataset_name, _dataset_path_lst in intensity_tune_dataset_path_lst[phase][pose].items()}\n",
    "\n",
    "                dataset_name_lst = list(dataset_lst.keys())\n",
    "\n",
    "                path_lst = { dataset_name: [] for dataset_name in dataset_lst.keys() }\n",
    "                label_lst = { dataset_name: [] for dataset_name in dataset_lst.keys() }\n",
    "                pred_lst = { dataset_name: [] for dataset_name in dataset_lst.keys() }\n",
    "                subject_lst = { dataset_name: [] for dataset_name in dataset_lst.keys() }\n",
    "\n",
    "                for line_id,line in enumerate(open(csv_filename)):\n",
    "                    if line_id == 0:\n",
    "                        continue\n",
    "\n",
    "                    path,label,pred = line.rstrip().split(\",\")\n",
    "                    label = int(label)\n",
    "                    pred = float(pred)\n",
    "                    subject = os.path.basename(path).split(\"_\")[2]\n",
    "\n",
    "                    dataset_name = None\n",
    "                    for _dataset_name, _dataset_path_lst in dataset_lst.items():\n",
    "                        for dataset_path in _dataset_path_lst:\n",
    "                            if path.find(dataset_path) == 0:\n",
    "                                dataset_name = _dataset_name\n",
    "                    if dataset_name is None:\n",
    "                        raise\n",
    "\n",
    "                    path_lst[dataset_name].append(path)\n",
    "                    label_lst[dataset_name].append(label)\n",
    "                    pred_lst[dataset_name].append(pred)\n",
    "                    subject_lst[dataset_name].append(subject)\n",
    "\n",
    "\n",
    "                output_csv_filename = dataset_lst_dirname + \"/AU_INT_CONV_%s_%s_%s.csv\" % (au_name,phase,pose)\n",
    "\n",
    "                if phase == \"Train\":\n",
    "\n",
    "                    filename_lst_cache_lst = {}\n",
    "\n",
    "                    for dataset_name in dataset_lst.keys():\n",
    "\n",
    "                        filename_lst_cache = {}\n",
    "\n",
    "                        for i in range(len(path_lst[dataset_name])):\n",
    "\n",
    "                            subject = subject_lst[dataset_name][i]\n",
    "                            label = label_lst[dataset_name][i]\n",
    "\n",
    "                            if not label in filename_lst_cache:\n",
    "                                filename_lst_cache[label] = {}\n",
    "\n",
    "                            if not subject in filename_lst_cache[label]:\n",
    "                                filename_lst_cache[label][subject] = []\n",
    "\n",
    "                            filename_lst_cache[label][subject].append( (path_lst[dataset_name][i], pred_lst[dataset_name][i]) )\n",
    "\n",
    "                        filename_lst_cache_lst[dataset_name] = filename_lst_cache\n",
    "\n",
    "\n",
    "                    fp = open(output_csv_filename,\"w\")\n",
    "\n",
    "                    counter = 0\n",
    "                    while counter < intensity_conv_dataset_num_lst[phase]:\n",
    "\n",
    "                        dataset_name = random.sample(dataset_name_lst,1)[0]\n",
    "                        filename_lst_cache = filename_lst_cache_lst[dataset_name]\n",
    "                        if len(list(filename_lst_cache.keys())) == 0:\n",
    "                            continue\n",
    "\n",
    "                        intensity = random.sample(list(filename_lst_cache.keys()),1)[0]                    \n",
    "\n",
    "                        subject = random.sample(sorted(list(filename_lst_cache[intensity].keys())),1)[0]\n",
    "\n",
    "                        path, pred = random.sample(filename_lst_cache[intensity][subject],1)[0]\n",
    "\n",
    "                        label = intensity\n",
    "\n",
    "                        fp.write(\"%s,%d,%f\\n\" % (path,label,pred))\n",
    "\n",
    "                        counter += 1\n",
    "\n",
    "\n",
    "                    fp.close()\n",
    "\n",
    "\n",
    "                else:            \n",
    "                    fp = open(output_csv_filename,\"w\")\n",
    "                    print(csv_filename,len(path_lst))\n",
    "\n",
    "                    for dataset_name in dataset_name_lst:\n",
    "                        n = intensity_conv_dataset_num_lst[phase] if intensity_conv_dataset_num_lst[phase] >= 0 else len(path_lst[dataset_name])\n",
    "                        n = min(len(path_lst[dataset_name]),n)\n",
    "                        \n",
    "                        print(au_name,phase,pose,dataset_name,\"sampling:\",n)\n",
    "                        \n",
    "                        for i in random.sample(list(range(len(path_lst[dataset_name]))),n):\n",
    "                            fp.write(\"%s,%d,%f\\n\" % (path_lst[dataset_name][i],label_lst[dataset_name][i],pred_lst[dataset_name][i]))\n",
    "\n",
    "                    fp.close()                \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T07:00:50.073351Z",
     "start_time": "2020-07-31T07:00:39.174031Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dataset\n",
    "class IntensityConvDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, csv_filename_all, csv_filename_target,pred_min,pred_max):\n",
    "        \n",
    "        self.subject_feature_lst = {}\n",
    "        self.pred_min = pred_min\n",
    "        self.pred_max = pred_max\n",
    "        \n",
    "        for line_id,line in enumerate(open(csv_filename_all)):\n",
    "\n",
    "            if line_id == 0:\n",
    "                continue\n",
    "\n",
    "            path,label,pred = line.rstrip().split(\",\")\n",
    "\n",
    "            subject = os.path.basename(path).split(\"_\")[2]\n",
    "            task = os.path.basename(path).split(\"_\")[3]\n",
    "            pose = os.path.basename(path).split(\"_\")[4]            \n",
    "\n",
    "            label = int(label)\n",
    "            pred = float(pred)\n",
    "\n",
    "            \n",
    "            if not subject in self.subject_feature_lst:\n",
    "                self.subject_feature_lst[subject] = {}\n",
    "            if not pose in self.subject_feature_lst[subject]:\n",
    "                self.subject_feature_lst[subject][pose] = {}\n",
    "            if not task in self.subject_feature_lst[subject][pose]:\n",
    "                self.subject_feature_lst[subject][pose][task] = { \"pred_lst\":[], \"basename_lst\":[], \"path_lst\":[], \"frame_id_lst\":[] }\n",
    "                \n",
    "            self.subject_feature_lst[subject][pose][task][\"pred_lst\"].append(pred)\n",
    "            self.subject_feature_lst[subject][pose][task][\"path_lst\"].append(path)\n",
    "            \n",
    "            basename = os.path.splitext(os.path.basename(path))[0]\n",
    "            self.subject_feature_lst[subject][pose][task][\"basename_lst\"].append(basename)\n",
    "            frame_id = int(basename.split(\"_\")[5][len(\"frame\"):])\n",
    "            self.subject_feature_lst[subject][pose][task][\"frame_id_lst\"].append(frame_id)\n",
    "            \n",
    "            \n",
    "\n",
    "        for subject in self.subject_feature_lst.keys():\n",
    "            for pose in self.subject_feature_lst[subject].keys():\n",
    "                for task in self.subject_feature_lst[subject][pose].keys():\n",
    "                    \n",
    "                    frame_id_lst_with_i = []\n",
    "                    \n",
    "                    for i in range(len(self.subject_feature_lst[subject][pose][task][\"frame_id_lst\"])):\n",
    "                        frame_id_lst_with_i.append((self.subject_feature_lst[subject][pose][task][\"frame_id_lst\"][i],i))\n",
    "                    \n",
    "                    frame_id_lst_with_i.sort()\n",
    "                    \n",
    "                    for key in self.subject_feature_lst[subject][pose][task].keys():\n",
    "                        self.subject_feature_lst[subject][pose][task][key] = [ self.subject_feature_lst[subject][pose][task][key][i] for frame_id,i in frame_id_lst_with_i ]\n",
    "                    \n",
    "\n",
    "\n",
    "        pred_lst_len = []\n",
    "        for subject in self.subject_feature_lst.keys():\n",
    "            for pose in self.subject_feature_lst[subject].keys():\n",
    "                for task in self.subject_feature_lst[subject][pose].keys():\n",
    "                    pred_lst_len.append(len(self.subject_feature_lst[subject][pose][task][\"pred_lst\"]))\n",
    "\n",
    "\n",
    "        pred_lst_max_len = max(pred_lst_len)\n",
    "        print(\"min:\",min(pred_lst_len),\"max:\",max(pred_lst_len),\"mean:\",np.mean(pred_lst_len),\"std:\",np.std(pred_lst_len))\n",
    "\n",
    "        frame_id_all_lst = []\n",
    "        for subject in self.subject_feature_lst.keys():\n",
    "            for pose in self.subject_feature_lst[subject].keys():\n",
    "                for task in self.subject_feature_lst[subject][pose].keys():\n",
    "                    frame_id_all_lst += self.subject_feature_lst[subject][pose][task][\"frame_id_lst\"]\n",
    "    \n",
    "\n",
    "        frame_id_min = min(frame_id_all_lst)\n",
    "        frame_id_max = max(frame_id_all_lst)\n",
    "        \n",
    "        for subject in self.subject_feature_lst.keys():\n",
    "            for pose in self.subject_feature_lst[subject].keys():\n",
    "                for task in self.subject_feature_lst[subject][pose].keys():\n",
    "                    self.subject_feature_lst[subject][pose][task][\"pred_lst_padding\"] = []\n",
    "                    mean_frame_width = 10\n",
    "                    value = np.mean(self.subject_feature_lst[subject][pose][task][\"pred_lst\"][-mean_frame_width:])\n",
    "                    self.subject_feature_lst[subject][pose][task][\"last_padding_value\"] = value\n",
    "                    \n",
    "                    target_frame_id_id = 0\n",
    "                    br = False\n",
    "                    for frame_id in range(frame_id_min,frame_id_max+1):\n",
    "                        \n",
    "                        if br:\n",
    "                            break\n",
    "                        \n",
    "                        while True:                           \n",
    "                            \n",
    "                            \n",
    "                            if target_frame_id_id >= len(self.subject_feature_lst[subject][pose][task][\"frame_id_lst\"]):\n",
    "                                br = True\n",
    "                                \n",
    "                                break\n",
    "                        \n",
    "                            target_frame_id = self.subject_feature_lst[subject][pose][task][\"frame_id_lst\"][target_frame_id_id]\n",
    "\n",
    " \n",
    "                            if frame_id == target_frame_id:\n",
    "                                self.subject_feature_lst[subject][pose][task][\"pred_lst_padding\"].append(self.subject_feature_lst[subject][pose][task][\"pred_lst\"][target_frame_id_id])\n",
    "                                break\n",
    "                            elif frame_id < target_frame_id:\n",
    "\n",
    "                                if len(self.subject_feature_lst[subject][pose][task][\"pred_lst_padding\"]) == 0:\n",
    "                                    value = np.mean(self.subject_feature_lst[subject][pose][task][\"pred_lst\"][0:mean_frame_width])\n",
    "                                else:\n",
    "                                    value = np.mean(self.subject_feature_lst[subject][pose][task][\"pred_lst_padding\"][-mean_frame_width:])\n",
    "\n",
    "                                self.subject_feature_lst[subject][pose][task][\"pred_lst_padding\"].append(value)\n",
    "                    \n",
    "                                break\n",
    "                            else:\n",
    "                                target_frame_id_id += 1\n",
    "                                \n",
    "\n",
    "        self.data_lst = []           \n",
    "        for line_id,line in enumerate(open(csv_filename_target)):          \n",
    "            \n",
    "            path,label,pred = line.rstrip().split(\",\")\n",
    "\n",
    "            subject = os.path.basename(path).split(\"_\")[2]\n",
    "            task = os.path.basename(path).split(\"_\")[3]\n",
    "            pose = os.path.basename(path).split(\"_\")[4]            \n",
    "\n",
    "            label = int(label)\n",
    "            pred = float(pred)\n",
    "            \n",
    "            self.data_lst.append( (path,subject,pose,task,pred,label) )\n",
    "\n",
    "\n",
    "        for subject in self.subject_feature_lst.keys():\n",
    "            for pose in self.subject_feature_lst[subject].keys():\n",
    "                for task in self.subject_feature_lst[subject][pose].keys():\n",
    "                    \n",
    "                    \n",
    "                    if len(self.subject_feature_lst[subject][pose][task][\"pred_lst\"]) <= 1:\n",
    "                        print(\"[WARN] no frame\",len(self.subject_feature_lst[subject][pose][task][\"pred_lst\"]),self.subject_feature_lst[subject][pose][task][\"pred_lst\"])\n",
    "                        self.subject_feature_lst[subject][pose][task][\"hist\"] = [0.0]*24\n",
    "                        self.subject_feature_lst[subject][pose][task][\"percentile\"] = [0.0]*11\n",
    "                        self.subject_feature_lst[subject][pose][task][\"path_min\"] = \"\"                        \n",
    "                        \n",
    "                        continue\n",
    "                        \n",
    "                    \n",
    "            \n",
    "                    hist_n = 6*4\n",
    "                    \n",
    "                    kde_model = gaussian_kde([ pred + (random.random()*2-1)*1.0e-10 for pred in self.subject_feature_lst[subject][pose][task][\"pred_lst\"]])\n",
    "                    kde_x = np.linspace(pred_min,pred_max,num=hist_n*10)\n",
    "                    kde_y = kde_model(kde_x)       \n",
    "                    \n",
    "\n",
    "                    kde_x_q = np.linspace(pred_min,pred_max,num=hist_n)\n",
    "                    kde_y_q = [ np.mean([ kde_y[i*10+j] for j in range(10)]) for i in range(hist_n) ]\n",
    "\n",
    "                    hist = list(kde_y_q)\n",
    "                    hist_total = sum(hist)\n",
    "                    if hist_total > 0:\n",
    "                        hist = [ a/hist_total for a in hist ]\n",
    "\n",
    "                    self.subject_feature_lst[subject][pose][task][\"hist\"] = hist\n",
    "\n",
    "                    \n",
    "                    # percentile\n",
    "                    self.subject_feature_lst[subject][pose][task][\"percentile\"] = []\n",
    "                    \n",
    "                    pred_lst_sorted = sorted(self.subject_feature_lst[subject][pose][task][\"pred_lst\"])\n",
    "                    \n",
    "                    for p in [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]: # 11\n",
    "                        self.subject_feature_lst[subject][pose][task][\"percentile\"].append((pred_lst_sorted[int((len(pred_lst_sorted)-1)*p)]-pred_min)/(pred_max-pred_min)*2-1)\n",
    "                        \n",
    "    \n",
    "        \n",
    "\n",
    "                    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_lst)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path,subject,pose,task,psedo_pred,label = self.data_lst[idx]\n",
    "\n",
    "        \n",
    "        feature = []\n",
    "        if \"hist\" in intensity_net_feature_lst:\n",
    "            feature += list(self.subject_feature_lst[subject][pose][task][\"hist\"])\n",
    "        if \"percentile\" in intensity_net_feature_lst:\n",
    "            feature += list(self.subject_feature_lst[subject][pose][task][\"percentile\"])\n",
    "       \n",
    "        return torch.tensor(feature), \\\n",
    "               torch.tensor([(psedo_pred-self.pred_min)/(self.pred_max-self.pred_min)*2-1]), \\\n",
    "               torch.tensor(label), \\\n",
    "               path, \\\n",
    "               None, \\\n",
    "               None, \\\n",
    "               None\n",
    "               \n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T07:00:50.073351Z",
     "start_time": "2020-07-31T07:00:39.174031Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pred_min,pred_max\n",
    "def get_pred_min_pred_max( intensity_tune_dataset_path_lst, au_name_lst, is_mini ):\n",
    "    \n",
    "    intensity_net_param_lst = {}\n",
    "\n",
    "    for au_name in au_name_lst:\n",
    "        \n",
    "        pred_min = 100000\n",
    "        pred_max = -100000\n",
    "    \n",
    "        for phase in [\"Train\"]:\n",
    "            for pose in intensity_tune_dataset_path_lst[phase].keys():\n",
    "\n",
    "                csv_filename_all =\"./pred_result/pred_result__%s_procrustes-intensity-%s_%s_psedo%s.csv\" % (au_name,pose,phase,\"_mini\" if is_mini else \"\")\n",
    "\n",
    "                for line_id,line in enumerate(open(csv_filename_all)):\n",
    "\n",
    "                    if line_id == 0:\n",
    "                        continue\n",
    "\n",
    "                    path,label,pred = line.rstrip().split(\",\")\n",
    "                    pred = float(pred)\n",
    "\n",
    "                    if pred_min > pred:\n",
    "                        pred_min = pred\n",
    "                    if pred_max < pred:\n",
    "                        pred_max = pred\n",
    "            \n",
    "        print(pred_min,pred_max)\n",
    "        intensity_net_param_lst[au_name] = { \"pred_min\":pred_min, \"pred_max\":pred_max }\n",
    "    \n",
    "    return intensity_net_param_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader_collate_fn(batch):\n",
    "    feature, psedo_pred, label, path, _, _, _ = list(zip(*batch))\n",
    "    feature = torch.stack(feature)\n",
    "    psedo_pred = torch.stack(psedo_pred)\n",
    "    label = torch.stack(label) \n",
    "    \n",
    "    return feature,psedo_pred,label,path,None,None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T07:00:50.073351Z",
     "start_time": "2020-07-31T07:00:39.174031Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_dataloaders_for_intensity_net( intensity_tune_dataset_path_lst, au_name_lst, phase_lst, phase_full_lst, intensity_net_param_lst, batch_size_int, is_mini ):\n",
    "\n",
    "    dataloaders_intensity_conv = {}\n",
    "\n",
    "    for au_name in au_name_lst:\n",
    "        pred_min = intensity_net_param_lst[au_name][\"pred_min\"]\n",
    "        pred_max = intensity_net_param_lst[au_name][\"pred_max\"]\n",
    "        dataloaders_intensity_conv[au_name] = {}\n",
    "        for phase in phase_lst + phase_full_lst:\n",
    "            dataloaders_intensity_conv[au_name][phase] = {}\n",
    "            for pose in intensity_tune_dataset_path_lst[phase].keys():\n",
    "\n",
    "                print(\"=========\")\n",
    "                print(phase,pose)\n",
    "\n",
    "                phase_tmp = phase\n",
    "                if phase_tmp == \"TrainFull\":\n",
    "                    phase_tmp = \"Train\"\n",
    "                if phase_tmp == \"ValidFull\":\n",
    "                    phase_tmp = \"Valid\"\n",
    "                if phase_tmp == \"TestFull\":\n",
    "                    phase_tmp = \"Test\"\n",
    "\n",
    "                csv_filename_all =\"./pred_result/pred_result__%s_procrustes-intensity-%s_%s_psedo%s.csv\" % (au_name,pose, phase_tmp, \"_mini\" if is_mini else \"\")\n",
    "                csv_filename_target =dataset_lst_dirname + \"/AU_INT_CONV_%s_%s_%s.csv\" % (au_name,phase,pose)\n",
    "\n",
    "                intensity_conv_dataset = IntensityConvDataset(csv_filename_all,csv_filename_target,pred_min,pred_max)\n",
    "                dataloader = torch.utils.data.DataLoader(intensity_conv_dataset, batch_size=batch_size_int, shuffle=True, num_workers=1,collate_fn=dataloader_collate_fn)\n",
    "\n",
    "                dataloaders_intensity_conv[au_name][phase][pose] = dataloader\n",
    "\n",
    "    return dataloaders_intensity_conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T07:00:50.099248Z",
     "start_time": "2020-07-31T07:00:50.084996Z"
    }
   },
   "outputs": [],
   "source": [
    "class IntensityNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(IntensityNet, self).__init__()\n",
    "\n",
    "        classifier_in_dim = 1\n",
    "        \n",
    "        if \"hist\" in intensity_net_feature_lst:\n",
    "            classifier_in_dim += 6*4\n",
    "        if \"percentile\" in intensity_net_feature_lst:\n",
    "            classifier_in_dim += 11\n",
    "\n",
    "        \n",
    "        self.img_features = models.vgg16(pretrained=True).features      \n",
    "        for name, param in self.img_features.named_parameters():\n",
    "            if name.find(\"0.\") == 0 or name.find(\"2.\") == 0:\n",
    "                print(\"freeze:\",name)\n",
    "                param.requires_grad = False\n",
    "\n",
    "\n",
    "        self.layers_classifier = []\n",
    "        self.layers_classifier.append(nn.Linear(in_features=classifier_in_dim, out_features=4096, bias=True))      \n",
    "        self.layers_classifier.append(nn.ReLU(inplace=True))\n",
    "        self.layers_classifier.append(nn.Dropout(p=0.5, inplace=False))\n",
    "        self.layers_classifier.append(nn.Linear(in_features=4096, out_features=4096, bias=True))\n",
    "        self.layers_classifier.append(nn.ReLU(inplace=True))\n",
    "        self.layers_classifier.append(nn.Dropout(p=0.5, inplace=False))        \n",
    "        self.layers_classifier.append(nn.Linear(in_features=4096, out_features=intensity_level, bias=True))\n",
    "        self.layers_classifier = nn.ModuleList(self.layers_classifier)\n",
    "        \n",
    "    def forward(self, x, feature, _, __):\n",
    "\n",
    "\n",
    "        feature = feature.float()\n",
    "\n",
    "              \n",
    "        x = torch.cat([x,feature],dim=1)\n",
    "\n",
    "        for i in range(len(self.layers_classifier)):\n",
    "            x = self.layers_classifier[i](x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T07:10:13.208859Z",
     "start_time": "2020-07-31T07:00:50.101203Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_intensity_net( intensity_tune_dataset_path_lst, dataloaders_intensity_conv, au_name_lst, phase_lst, device, epoch_num ):\n",
    "\n",
    "    for au_name in au_name_lst:\n",
    "\n",
    "        intensity_net = IntensityNet()\n",
    "        intensity_net.to(device)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(intensity_net.parameters(), lr=0.00005)\n",
    "\n",
    "        epoch_score_lst = { phase:{ pose:{ measurement_name:[], \"LOSS\":[] } for pose in intensity_tune_dataset_path_lst[phase].keys() } for phase in phase_lst }\n",
    "\n",
    "        for epoch in range(epoch_num):\n",
    "            print(\"================\")\n",
    "            print(\"epoch\",epoch)\n",
    "\n",
    "            for phase in phase_lst:\n",
    "\n",
    "                if phase == \"Train\":\n",
    "                    intensity_net.train()\n",
    "                else:\n",
    "                    intensity_net.eval()         \n",
    "\n",
    "                print(\"------------\")\n",
    "                print(au_name,phase)\n",
    "\n",
    "                pred_lst = []\n",
    "                label_lst = []\n",
    "                path_lst = []\n",
    "\n",
    "                icc_lst = []\n",
    "\n",
    "                for pose in intensity_tune_dataset_path_lst[phase].keys():\n",
    "                    print(\"phase\",phase,\"pose\",pose)\n",
    "\n",
    "                    pred_lst_pose = []\n",
    "                    psedo_pred_lst_pose = []\n",
    "                    label_lst_pose = []\n",
    "                    path_lst_pose = []\n",
    "\n",
    "                    running_loss = 0\n",
    "                    running_corrects = 0\n",
    "                    total_input_size = 0\n",
    "\n",
    "\n",
    "                    for feature_batch, psedo_pred_batch, label_batch, path_batch, _, _ in dataloaders_intensity_conv[au_name][phase][pose]:\n",
    "\n",
    "                        feature_batch = feature_batch.to(device)\n",
    "                        psedo_pred_batch = psedo_pred_batch.to(device)\n",
    "                        label_batch_org = label_batch\n",
    "                        label_batch = label_batch.to(device)\n",
    "\n",
    "\n",
    "                        optimizer.zero_grad()\n",
    "\n",
    "                        with torch.set_grad_enabled(phase == 'Train'):\n",
    "\n",
    "                            output_batch = intensity_net.forward(psedo_pred_batch,feature_batch,_,_)\n",
    "                            _, pred_batch = torch.max(output_batch, 1)\n",
    "\n",
    "                            loss = criterion(output_batch, label_batch)\n",
    "\n",
    "                            if phase == 'Train':\n",
    "                                loss.backward()\n",
    "                                optimizer.step()\n",
    "\n",
    "                            running_loss += loss.item() * feature_batch.size(0)\n",
    "                            running_corrects += torch.sum(pred_batch == label_batch.data)\n",
    "                            total_input_size += feature_batch.size(0)\n",
    "\n",
    "                        pred_lst_pose += list(pred_batch.to('cpu').detach().numpy().copy())\n",
    "                        psedo_pred_lst_pose += list(psedo_pred_batch.to('cpu').detach().numpy().copy())\n",
    "                        label_lst_pose += list(label_batch_org.numpy().copy())\n",
    "                        path_lst_pose += list(path_batch)\n",
    "\n",
    "\n",
    "                    cur_icc = get_measurement_score(label_lst_pose,pred_lst_pose)\n",
    "                    print(measurement_name,cur_icc)\n",
    "                    icc_lst.append(cur_icc)\n",
    "\n",
    "                    loss_pose = running_loss / total_input_size\n",
    "\n",
    "                    pred_lst += pred_lst_pose\n",
    "                    label_lst += label_lst_pose\n",
    "                    path_lst += path_lst_pose\n",
    "\n",
    "                    epoch_score_lst[phase][pose][measurement_name].append(cur_icc)\n",
    "                    epoch_score_lst[phase][pose][\"LOSS\"].append(loss_pose)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                print(measurement_name,\"(MEAN)\",np.average(icc_lst))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # save\n",
    "                fp = open(\"./log/intensity_net_epoch_score_lst_%s_%s.json\" % (au_name, \"posesall\") ,\"w\")\n",
    "                json.dump( epoch_score_lst, fp )\n",
    "                fp.close()\n",
    "\n",
    "\n",
    "                # save\n",
    "                if phase == \"Train\":\n",
    "                    torch.save(intensity_net.state_dict(), \"./model/intensity_net_%s_epoch_%d_%s.pt\" % (au_name, epoch, \"posesall\" ) )\n",
    "\n",
    "        del intensity_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intensity_net_best_epoch_lst( dataset_path_lst, au_name_lst ):\n",
    "    \n",
    "    intensity_net_best_epoch_lst = {}\n",
    "\n",
    "    for au_name in au_name_lst:\n",
    "        print(\"=============\")\n",
    "        print(au_name)\n",
    "\n",
    "        filename = \"./log/intensity_net_epoch_score_lst_%s_%s.json\" % (au_name,\"posesall\")\n",
    "\n",
    "        intensity_net_best_epoch_lst[au_name] = get_best_epoch( filename, \"LOSS\", \"LOWER\", dataset_path_lst )\n",
    "        show_score_lst( filename, dataset_path_lst )\n",
    "    \n",
    "    return intensity_net_best_epoch_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict intensity of all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_all_testset( dataloaders_intensity_conv, intensity_tune_dataset_path_lst, intensity_net_best_filename_lst, au_name_lst, device, phase_full_lst ):\n",
    "\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    for au_name in au_name_lst:\n",
    "\n",
    "        intensity_net = IntensityNet()\n",
    "        intensity_net.load_state_dict(torch.load(intensity_net_best_filename_lst[au_name]))\n",
    "        intensity_net.eval()\n",
    "        intensity_net.to(device)\n",
    "\n",
    "        for phase in phase_full_lst:\n",
    "\n",
    "            print(\"------------\")\n",
    "            print(au_name,phase)\n",
    "\n",
    "            pred_lst = []\n",
    "            label_lst = []\n",
    "            path_lst = []\n",
    "\n",
    "            icc_lst = []\n",
    "\n",
    "            for pose in intensity_tune_dataset_path_lst[phase].keys():\n",
    "                print(\"phase\",phase,\"pose\",pose)\n",
    "\n",
    "                pred_lst_pose = []\n",
    "                psedo_pred_lst_pose = []\n",
    "                label_lst_pose = []\n",
    "                path_lst_pose = []\n",
    "                pred_prob_lst_pose = []\n",
    "\n",
    "                running_corrects = 0\n",
    "                total_input_size = 0\n",
    "\n",
    "\n",
    "                for feature_batch, psedo_pred_batch, label_batch, path_batch, _, _ in dataloaders_intensity_conv[au_name][phase][pose]:\n",
    "\n",
    "                    feature_batch = feature_batch.to(device)\n",
    "                    psedo_pred_batch = psedo_pred_batch.to(device)\n",
    "                    label_batch_org = label_batch\n",
    "                    label_batch = label_batch.to(device)\n",
    "\n",
    "\n",
    "                    with torch.set_grad_enabled(False):\n",
    "\n",
    "                        output_batch = intensity_net.forward(psedo_pred_batch,feature_batch,_,_)\n",
    "                        _, pred_batch = torch.max(output_batch, 1)\n",
    "                        pred_prob_batch = softmax(output_batch)\n",
    "\n",
    "                        running_corrects += torch.sum(pred_batch == label_batch.data)\n",
    "                        total_input_size += feature_batch.size(0)\n",
    "\n",
    "                    pred_lst_pose += list(pred_batch.to('cpu').detach().numpy().copy())\n",
    "                    psedo_pred_lst_pose += list(psedo_pred_batch.to('cpu').detach().numpy().copy())\n",
    "                    label_lst_pose += list(label_batch_org.numpy().copy())\n",
    "                    path_lst_pose += list(path_batch)                \n",
    "                    pred_prob_lst_pose += list(pred_prob_batch.to('cpu').detach().numpy().copy())\n",
    "\n",
    "\n",
    "                # eval\n",
    "                cur_icc = get_measurement_score(label_lst_pose,pred_lst_pose)\n",
    "                print(measurement_name,cur_icc)\n",
    "                print(\"len\",len(label_lst_pose))\n",
    "                icc_lst.append(cur_icc)\n",
    "\n",
    "                pred_lst += pred_lst_pose\n",
    "                label_lst += label_lst_pose\n",
    "                path_lst += path_lst_pose\n",
    "\n",
    "\n",
    "                # save\n",
    "                fp_pred_result = open(\"./pred_result/pred_result__%s_procrustes-intensity-%s_%s.csv\" % (au_name,pose,phase),\"w\")\n",
    "                fp_pred_result.write(\"path,label,pred\\n\")\n",
    "\n",
    "                for i in range(len(path_lst_pose)):\n",
    "                    fp_pred_result.write(\"%s,%d,%d\\n\"%(path_lst_pose[i],label_lst_pose[i],pred_lst_pose[i]))\n",
    "\n",
    "                fp_pred_result.close()\n",
    "\n",
    "\n",
    "                # save\n",
    "                fp_pred_result_raw = open(\"./pred_result/pred_result__%s_procrustes-intensity-%s_%s_psedo_norm.csv\" % (au_name,pose,phase),\"w\")\n",
    "                fp_pred_result_raw.write(\"path,label,pred\\n\")\n",
    "\n",
    "                for i in range(len(path_lst_pose)):\n",
    "                    fp_pred_result_raw.write(\"%s,%d,%f\\n\"%(path_lst_pose[i],label_lst_pose[i],psedo_pred_lst_pose[i]))\n",
    "                fp_pred_result_raw.close()\n",
    "\n",
    "                # save\n",
    "                fp_pred_result_prob = open(\"./pred_result/pred_result__%s_procrustes-intensity-%s_%s_prob.csv\" % (au_name,pose,phase),\"w\")\n",
    "                fp_pred_result_prob.write(\"path,label,%s\\n\" % \",\".join([ \"prob_%d\" % intensity for intensity in range(intensity_level) ]) )\n",
    "\n",
    "                for i in range(len(path_lst_pose)):\n",
    "                    fp_pred_result_prob.write(\"%s,%d\"%(path_lst_pose[i],label_lst_pose[i]))\n",
    "                    for intensity in range(intensity_level):\n",
    "                        fp_pred_result_prob.write(\",%.10f\"%pred_prob_lst_pose[i][intensity])\n",
    "                    fp_pred_result_prob.write(\"\\n\")\n",
    "                fp_pred_result_prob.close()\n",
    "\n",
    "\n",
    "\n",
    "            # eval\n",
    "            print(measurement_name,\"(MEAN)\",np.average(icc_lst))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Free memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def memory_free_intensity_net_dataloaders( dataloaders_intensity_conv ):\n",
    "    for au_name in dataloaders_intensity_conv.keys():\n",
    "        for phase in dataloaders_intensity_conv[au_name].keys():\n",
    "            for pose in list(dataloaders_intensity_conv[au_name][phase].keys()):\n",
    "                del dataloaders_intensity_conv[au_name][phase][pose]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_summary_log( fp_summary, pair_net_epoch_score_lst, au_name, phase_lst, network_name, best_epoch ):\n",
    "    \n",
    "    best_validation_score = None\n",
    "    \n",
    "    for phase in phase_lst:\n",
    "        measure_lst = list(pair_net_epoch_score_lst[phase].values())[0].keys()\n",
    "        for measure in measure_lst:\n",
    "            epoch_lst = range(len(list(list(pair_net_epoch_score_lst[phase].values())[0].values())[0]))\n",
    "            for epoch in epoch_lst:\n",
    "                pose_lst = list(pair_net_epoch_score_lst[phase].keys())\n",
    "                \n",
    "                best_epoch_flag = 1 if best_epoch == epoch else 0\n",
    "                \n",
    "                score_all_pose_lst = []\n",
    "                for pose in pose_lst:                    \n",
    "                    score = pair_net_epoch_score_lst[phase][pose][measure][epoch]\n",
    "                    score_all_pose_lst.append(score)\n",
    "                    \n",
    "                    fp_summary.write(\"%s,%d,%s,%s,%s,%d,%d,%s,%f\\n\"%(au_name,trial,network_name,phase,pose,epoch,best_epoch_flag,measure,score))\n",
    "\n",
    "                fp_summary.write(\"%s,%d,%s,%s,%s,%d,%d,%s,%f\\n\"%(au_name,trial,network_name,phase,\"pose-mean\",epoch,best_epoch_flag,measure,np.mean(score_all_pose_lst)))\n",
    "                \n",
    "                if phase == \"Valid\" and measure == \"LOSS\" and epoch == best_epoch:\n",
    "                    best_validation_score = np.mean(score_all_pose_lst)\n",
    "\n",
    "    fp_summary.flush()\n",
    "    \n",
    "    return best_validation_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_validation_loss_lst = { au_name:[] for au_name in au_name_lst }\n",
    "\n",
    "fp_summary = open(\"./log/summary.csv\",\"w\")\n",
    "fp_summary.write(\"au_name,trial,network_name,phase,pose,epoch,best_epoch_flag,measure,score\\n\")\n",
    "\n",
    "for trial in range(trial_num):\n",
    "    \n",
    "    print(\"@@@@@@@@@@@@@@@@@@@@@@@@@@@@\")\n",
    "    print(\"trial:\",trial)\n",
    "\n",
    "    # train pseudo-intensity model\n",
    "    dataloaders = get_dataloaders_for_pairwise_net( dataset_path_lst, dataset_lst_dirname, au_name_lst, phase_lst, batch_size_bin )\n",
    "    train_pairwise_net( dataloaders, dataset_path_lst, au_name_lst, phase_lst, device, train_size_per_epoch )\n",
    "    pairwise_net_best_epoch_lst = get_pairwise_net_best_epoch_lst( dataset_path_lst, au_name_lst )\n",
    "    memory_free_pairwise_net_dataloaders( dataloaders )\n",
    "    \n",
    "    # copy model of best_epoch\n",
    "    pair_net_best_filename_lst = {}\n",
    "    for au_name in au_name_lst:\n",
    "        shutil.copyfile( \"./model/pair_net_%s_epoch_%d_%s.pt\" % (au_name, pairwise_net_best_epoch_lst[au_name], \"posesall\" ),\n",
    "                         \"./model/pair_net_%s_epoch_best_%s_trial_%d.pt\" % (au_name, \"posesall\", trial ))\n",
    "        pair_net_best_filename_lst[au_name] = \"./model/pair_net_%s_epoch_best_%s_trial_%d.pt\" % (au_name, \"posesall\", trial )\n",
    "    \n",
    "    # predict pseudo-intensity\n",
    "    dataloaders_intensity_tune = get_dataloaders_for_intensity_tune( intensity_tune_dataset_path_lst, au_name_lst, phase_lst, batch_size_int, True )\n",
    "    predict_pairwise_net( pair_net_best_filename_lst, intensity_tune_dataset_path_lst, dataloaders_intensity_tune, au_name_lst, phase_lst, device, True )\n",
    "    memeory_free_dataloaders_intensity_tune( dataloaders_intensity_tune )\n",
    "    \n",
    "\n",
    "    # train mapping model\n",
    "    make_dataset_for_intensity_net( intensity_tune_dataset_path_lst, intensity_conv_dataset_num_lst, dataset_lst_dirname, au_name_lst, phase_lst, phase_full_lst, True )\n",
    "    intensity_net_param_lst = get_pred_min_pred_max( intensity_tune_dataset_path_lst, au_name_lst, True )\n",
    "    dataloaders_intensity_conv = get_dataloaders_for_intensity_net( intensity_tune_dataset_path_lst, au_name_lst, phase_lst, phase_full_lst, intensity_net_param_lst, batch_size_int, True )\n",
    "    train_intensity_net( intensity_tune_dataset_path_lst, dataloaders_intensity_conv, au_name_lst, phase_lst, device, epoch_num )\n",
    "    intensity_net_best_epoch_lst = get_intensity_net_best_epoch_lst( dataset_path_lst, au_name_lst )\n",
    "    memory_free_intensity_net_dataloaders( dataloaders_intensity_conv )\n",
    "    \n",
    "    # copy model of best_epoch\n",
    "    for au_name in au_name_lst:\n",
    "        shutil.copyfile( \"./model/intensity_net_%s_epoch_%d_%s.pt\" % (au_name, intensity_net_best_epoch_lst[au_name],\"posesall\"),\n",
    "                         \"./model/intensity_net_%s_epoch_best_%s_trial_%d.pt\" % (au_name, \"posesall\", trial ))\n",
    "\n",
    "    # copy parameters\n",
    "    fp = open(\"./model/intensity_net_param_lst_trial_%d.json\" % trial,\"w\")\n",
    "    json.dump( intensity_net_param_lst, fp )\n",
    "    fp.close()\n",
    "        \n",
    "    # save result\n",
    "    for au_name in au_name_lst:\n",
    "\n",
    "        network_name = \"pairwise\"\n",
    "        best_epoch = pairwise_net_best_epoch_lst[au_name]\n",
    "        fp = open(\"./log/pair_net_epoch_score_lst_%s_posesall.json\" % au_name)\n",
    "        pair_net_epoch_score_lst = json.load( fp )\n",
    "        fp.close()\n",
    "        write_summary_log( fp_summary, pair_net_epoch_score_lst, au_name, phase_lst, network_name, best_epoch )\n",
    "\n",
    "\n",
    "        network_name = \"intensity\"\n",
    "        best_epoch = intensity_net_best_epoch_lst[au_name]\n",
    "        fp = open(\"./log/intensity_net_epoch_score_lst_%s_posesall.json\" % au_name)\n",
    "        intensity_net_epoch_score_lst = json.load( fp )\n",
    "        fp.close()\n",
    "        best_validation_loss = write_summary_log( fp_summary, intensity_net_epoch_score_lst, au_name, phase_lst, network_name, best_epoch )\n",
    "\n",
    "        best_validation_loss_lst[au_name].append(best_validation_loss)\n",
    "\n",
    "fp_summary.close()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best trial\n",
    "best_trial_lst = {}\n",
    "for au_name in au_name_lst:\n",
    "    best_trial = np.argmin(best_validation_loss_lst[au_name])\n",
    "    best_trial_lst[au_name] = best_trial\n",
    "best_trial_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pair_net_best_filename_lst = {}\n",
    "for au_name in au_name_lst:\n",
    "    pair_net_best_filename_lst[au_name] = \"./model/pair_net_%s_epoch_best_%s_trial_%d.pt\" % (au_name, \"posesall\", best_trial_lst[au_name] )\n",
    "\n",
    "# Predict pseudo-intensity(mini)\n",
    "dataloaders_intensity_tune = get_dataloaders_for_intensity_tune( intensity_tune_dataset_path_lst, au_name_lst, [ phase.replace(\"Full\",\"\") for phase in phase_full_lst], batch_size_int, True )\n",
    "predict_pairwise_net( pair_net_best_filename_lst, intensity_tune_dataset_path_lst, dataloaders_intensity_tune, au_name_lst, [ phase.replace(\"Full\",\"\") for phase in phase_full_lst], device, True )\n",
    "memeory_free_dataloaders_intensity_tune( dataloaders_intensity_tune )\n",
    "\n",
    "# Predict pseudo-intensity\n",
    "dataloaders_intensity_tune = get_dataloaders_for_intensity_tune( intensity_tune_dataset_path_lst, au_name_lst, [ phase.replace(\"Full\",\"\") for phase in phase_full_lst], batch_size_int, False )\n",
    "predict_pairwise_net( pair_net_best_filename_lst, intensity_tune_dataset_path_lst, dataloaders_intensity_tune, au_name_lst, [ phase.replace(\"Full\",\"\") for phase in phase_full_lst], device, False )\n",
    "memeory_free_dataloaders_intensity_tune( dataloaders_intensity_tune )\n",
    "\n",
    "intensity_net_best_filename_lst = {}\n",
    "for au_name in au_name_lst:\n",
    "    intensity_net_best_filename_lst[au_name] = \"./model/intensity_net_%s_epoch_best_%s_trial_%d.pt\" % (au_name, \"posesall\", best_trial_lst[au_name] )\n",
    "\n",
    "    \n",
    "fp = open(\"./model/intensity_net_param_lst_trial_%d.json\" % best_trial_lst[au_name])\n",
    "intensity_net_param_lst = json.load( fp )\n",
    "fp.close()\n",
    "    \n",
    "# Predict intensity\n",
    "make_dataset_for_intensity_net( intensity_tune_dataset_path_lst, intensity_conv_dataset_num_lst, dataset_lst_dirname, au_name_lst, [], phase_full_lst, False )\n",
    "dataloaders_intensity_conv = get_dataloaders_for_intensity_net( intensity_tune_dataset_path_lst, au_name_lst, [], phase_full_lst, intensity_net_param_lst, batch_size_int, True )\n",
    "predict_all_testset( dataloaders_intensity_conv, intensity_tune_dataset_path_lst, intensity_net_best_filename_lst, au_name_lst, device, phase_full_lst )\n",
    "memory_free_intensity_net_dataloaders( dataloaders_intensity_conv )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "188.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
